[
{
	"uri": "/",
	"title": "Amazon Kinesis",
	"tags": [],
	"description": "",
	"content": "Real Time Streaming with Amazon Kinesis Overall In this lab, you\u0026rsquo;ll learn the basics and practice streaming data with Amazon Kinesis. Perform producing and consuming data to understand the end-to-end process of real-time data ingestion, processing, and analysis using Amazon Kinesis streams and its associated services.\nContent Introduction Preparation Producing Data Consuming Data Consuming Data with Kinesis Data Firehose Cleanup Resources "
},
{
	"uri": "/4-consumedatainkds/4.1-consumerwithlambda/",
	"title": "Consume data with Aws Lambda",
	"tags": [],
	"description": "",
	"content": "AWS Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, and logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.\nIn this step, we will use AWS Lambda to consume data from Kinesis. We need to create a Lambda function and add a trigger from Kinesis.\nCreate Lambda function Go to the AWS Lambda Console. Select Functions. Click Create function. In Create function. Enter kinesisConsumeData. At Runtime, select python 3.12 (newest version). At Change default execution role, select Use a existing role. Search and select kinesis-lambda-lab-role. Scroll down and click Create function. In kinesisConsumeData function. Click Upload from and select .zip file. Download this zip file which has the Lambda consumer and the required packages into it to your machine and upload to Lambda function. We need to add an environment variable dynamoDBTableName with a value that is the name of the DynamoDB table you created.\nAdd Kinesis Trigger. Access to Lambda function. Select Add Trigger. In Add trigger. Select Kinesis. Choose kinesis-stream. Now, we will config like this: Write data to Kinesis Data Stream. Access into Cloud9 Environment. We will use python CDK to write data to the Kinesis Data Stream. Download the code here and upload to the Cloud9 Environment. We will run the command pip install boto3 to install boto3 library. After that, we need run the command python3 kds-py-sdk.py to write data to the Kinesis Data Stream. The result will look like this: Now, we will go to the CloudWatch logs to see what happen when we write data to the Kinesis Data Stream. Go the the Lambda function. Select Monotor. Click View in CloudWatch logs. In CloudWatch logs, we will see like this: Go to the AWS DynamoDB Console. Select Explore items. Choose kinesisAggs. "
},
{
	"uri": "/2-prerequiste/2.1-createcloud9/",
	"title": "Create Cloud9 Environment",
	"tags": [],
	"description": "",
	"content": "AWS Cloud9 is an integrated development environment, or IDE. The AWS Cloud9 IDE offers a rich code-editing experience with support for several programming languages and runtime debuggers, and a built-in terminal. It contains a collection of tools that you use to code, build, run, test, and debug software, and helps you release software to the cloud.\nIn this step, we need to set up a Cloud9 environment to run Python and JavaScript code, along with the Kinesis Producer Library and Kinesis Client Library.\nCreate Cloud9 Environment Go to the AWS Cloud9 Console. Select My environments. Click Create environment. In Create environment. Enter kinesis-env. Select New EC2 instance. At New EC2 instance, select t2.micro. Scroll down and click Create. After completing to creation of the Cloud9 environment, we can click Open to access the Cloud9 environment. Run the command sudo yum install maven -y to install Maven.\n"
},
{
	"uri": "/5-consumerwithkdf/5.1-createlambda/",
	"title": "Create Lambda function",
	"tags": [],
	"description": "",
	"content": "In this step, we need to create a Lambda function to transform data that Kinesis Data Firehose received from Kinesis Data Stream.\nCreate Kinesis Data Firehose. Go to the AWS Lambda Console Select Functions. Click Create function. In Create function. Enter dataTransformHandler. At Runtime, choose python 3.12 (newst version). Click Next. After creating successfully, we will paste the below code into the Lambda function that we created: import base64 import json print(\u0026#39;Loading function\u0026#39;) def lambda_handler(event, context): output = [] for record in event[\u0026#39;records\u0026#39;]: print(record[\u0026#39;recordId\u0026#39;]) # Decode the base64 encoded data and convert to string payload = base64.b64decode(record[\u0026#39;data\u0026#39;]).decode(\u0026#39;utf-8\u0026#39;) # Convert the string payload to a JSON object reading = json.loads(payload) # Add additional column \u0026#39;source\u0026#39; reading[\u0026#39;source\u0026#39;] = \u0026#39;NYCTAXI\u0026#39; # Encode the modified JSON object to base64 output_data = base64.b64encode(json.dumps(reading).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;) # Prepare the output record output_record = { \u0026#39;recordId\u0026#39;: record[\u0026#39;recordId\u0026#39;], \u0026#39;result\u0026#39;: \u0026#39;Ok\u0026#39;, \u0026#39;data\u0026#39;: output_data } output.append(output_record) print(\u0026#39;Successfully processed {} records.\u0026#39;.format(len(event[\u0026#39;records\u0026#39;]))) return {\u0026#39;records\u0026#39;: output} After that, we need to adjust the timeout of the Lambda function to 1 minute. In Lambda function. Select Configuration. Select General configuration. Click Edit. In Edit basic settings. At Timeout, fill out 1 minute. "
},
{
	"uri": "/4-consumedatainkds/4.3-consumerwithamazonmdf/4.3.1-createopensearchservice/",
	"title": "Create OpenSearch Service",
	"tags": [],
	"description": "",
	"content": "OpenSearch is a fully open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis.\nAmazon OpenSearch Service provisions all the resources for your OpenSearch cluster and launches it. It also automatically detects and replaces failed OpenSearch Service nodes, reducing the overhead associated with self-managed infrastructures. You can scale your cluster with a single API call or a few clicks in the console.\nIn this step, we need to create a OpenSearch Service to store data that Amazon Managed Service for Apache Flink received from Kinesis Data Stream.\nCreate OpenSearch Service. Go to the AWS OpenSearch Service Console. Select Dashboard. Select Create domain. At Templates, select Dev/test. Because in this lab, we only need to handle a little bit of data, we don\u0026rsquo;t need to create a production environment. We just need a dev/test environment to save costs.\nIn Create domain. Enter kinesis-opensearch-service. Select Standard create. Following by the below config: After these teps, we will click Create to create a OpenSearch Service Domain. Wait about 15 minutes, and we will see the kinesis-opensearch-service.\n"
},
{
	"uri": "/3-writedataintokds/3.3-producerwithstudionotebook/3.3.1-createstudionotebook/",
	"title": "Create Studio Notebook",
	"tags": [],
	"description": "",
	"content": "AWS Glue is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources. You can use it for analytics, machine learning, and application development. It also includes additional productivity and data ops tooling for authoring, running jobs, and implementing business workflows.\nIn this step, we will create a Studio notebook and a AWS Glue Database to start write data to the Kinesis Data Stream.\nCreate Glue Database Go to the AWS Kinesis Console. Select Databases. Enter kinesis-glue-db. Click Create database. Create Studio notebooks. Go to the AWS Kinesis Console. Select Managed Apache Flink. In Managed Apache Flink. Select Studio notebooks. Click Create Studio notebook. In Create Studio notebook. Select Create with custom settings. Enter kinesis-studio. Scroll down and click Next. In IAM permissions step. Select Choose from IAM roles that Managed Service for Apache Flink can assume. At Service role, select kinesis-studio-lab-role. At AWS Glue database, select kinesis-glue-db. Click Next. In the remaining steps, we select Next and Create Studio notebook.\nAfter creating successfully, we will click Run to run Studio notebook.\nAfter running successfully, we will click Open in Apache Zeppelin to access to the Studio notebook throughout the Apache Zeppelin.\nThe console of Apache Zeppelin will look like this.\n"
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website click streams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\nYou can use Amazon Kinesis Data Streams to ingest large amounts of data in real-time, to durably store the data, and make the data available for consumption. We can learn the basic knowledge of KDS:\nThe unit of data stored by Kinesis Data Streams is a data Record.\nThe data records in a data stream are distributed into Shards. A shard has a sequence of data records in a stream. When you create a stream, you specify the number of shards for the stream. The total capacity of a stream is the sum of the capacities of its shards. You can increase or decrease the number of shards in a stream as needed. However, you are charged on a per-shard basis. For information about the capacities and limits of a shard, see Kinesis Data Streams Limits.\nApplications or systems that generate and send data to Kinesis Data Streams are called Producers. Producers can include Amazon Kinesis Data Streams API, the Amazon Kinesis Producer Library (KPL), the Amazon Kinesis Agent — a pre-built Java application designed for easy data collection and transmission to your Amazon Kinesis stream, Lambda, Amazon DynamoDB, Amazon Aurora, Amazon CloudWatch, AWS IoT Core, and more.\nApplications or systems that retrieve data from Kinesis Data Streams for processing, analysis, or storage are called Consumers. Consumers can include AWS Lambda, Amazon Managed Service for Apache Flink, AWS Glue Streaming, Amazon Kinesis Client Library (KCL) - pre-built library that helps you easily build Amazon Kinesis applications for reading and processing data from an Amazon Kinesis data stream, Kinesis Data Firehose, and more.\nKinesis Data Firehose can send data records to various destinations, including Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, DynamoDB and any HTTP endpoint that is owned by you or any of your third-party service providers.\nThe basic architecture of Amazon Kineses Data Stream will be as follow:\nIn this lab, you’ll learn the basics and practice of Real-Time Streaming with Amazon Kinesis. We\u0026rsquo;ll delve into the intricacies of setting up Kinesis Data Streams (KDS), understanding the significance of shards, and configuring stream properties. You\u0026rsquo;ll discover the various methods to efficiently put data into KDS, leveraging tools like AWS SDK and AWS CLI. Additionally, we\u0026rsquo;ll explore the essentials of consuming data from KDS, implementing robust consumers, and ensuring seamless data processing. By the end of this lab, you\u0026rsquo;ll have a comprehensive understanding of Amazon Kinesis and its pivotal role in building scalable and responsive data streaming applications.\n"
},
{
	"uri": "/3-writedataintokds/3.1-producerwithsdk/",
	"title": "Write data with Kinesis SDK",
	"tags": [],
	"description": "",
	"content": "AWS Kinesis SDK is a software development kits (SDKs) provided by Amazon Web Services (AWS) to interact with Amazon Kinesis services.\nIn this step, we\u0026rsquo;ll use AWS SDK for Javascript to write data to the Kinesis Data Stream.\nCreate Javascript file Go to the AWS Cloud9 Console. Select My environments. Click Create environment. In Cloud9 environment. Select File. Select New From Templete. Select Javascript File. Copy and paste this code into Javascript file const { KinesisClient, PutRecordCommand } = require(\u0026#39;@aws-sdk/client-kinesis\u0026#39;); const REGION = \u0026#39;ap-southeast-1\u0026#39;; const STREAM_NAME = \u0026#39;kinesis-stream\u0026#39;; const config = { region: REGION, }; const client = new KinesisClient(config); const TOTAL_RECORDS = 20; const streamName = STREAM_NAME; const generateData = (i) =\u0026gt; { return { order_id: `ORD${i}`, order_user_id: `USR${i}`, order_items: [ { item_id: `ITEM00${i}`, item_name: `Product Name ${i}`, quantity: i + 1, price: (i + 1) * 10.99, total_price: (i + 1) * 10.99 * (i + 1), }, { item_id: `ITEM00${i + 1}`, item_name: `Product Name ${i + 1}`, quantity: i + 2, price: (i + 2) * 15.5, total_price: (i + 2) * 15.5 * (i + 2), }, ], order_date: \u0026#39;2024-04-17\u0026#39;, order_total_amount: (i + 1) * 10.99 * (i + 1) + (i + 2) * 15.5 * (i + 2), order_status: \u0026#39;Pending\u0026#39;, }; }; for (let i = 0; i \u0026lt; TOTAL_RECORDS; i++) { const jsonData = generateData(i); const data = JSON.stringify(jsonData); const partitionKey = `PartitionKey-${i}`; const input = { StreamName: streamName, Data: Buffer.from(data), PartitionKey: partitionKey, }; const command = new PutRecordCommand(input); (async () =\u0026gt; { try { const response = await client.send(command); console.log(`Record ${i} sent:`, response); } catch (error) { console.error(`Error sending record ${i}:`, error); } })(); } Save the code to a file named kds-js-sdk.js or any other name you prefer. Click Save. After that, we will run the command bash npm i @aws-sdk/client-kinesis to install AWS SDK for Kinesis.\nWe run the command node kds-js-sdk.js to write data to the Kinesis Data Stream.\nMake sure to edit REGION and STREAM_NAME to match with your config.\nWe can check the success of putting data by accessing the Kinesis Data Stream. Select Monitor. In Incoming data and PutRecords, we\u0026rsquo;ll see a little bit of data coming into Kinesis. Because we only put 20 records, it can be very small. If you\u0026rsquo;re not familiar with Javascript, you can download the Python code here and try to use that.\n"
},
{
	"uri": "/4-consumedatainkds/4.2-consumerwithkcl/",
	"title": "Consume data with Kinesis Library Client",
	"tags": [],
	"description": "",
	"content": "Kinesis Client Library for data analysis, archival, real-time dashboards, and much more. While you can use Amazon Kinesis API functions to process stream data directly, the KCL takes care of many complex tasks associated with distributed processing and allows you to focus on the record processing logic. For example, the KCL can automatically load balance record processing across many instances, allow the user to checkpoint records that are already processed, and handle instance failures. The KCL acts as an intermediary between your record processing logic and Kinesis Data Streams.\nIn this step, we will use the Kinesis Library Client to consume data from Kinesis.\nCreate Consumer code. Go to the AWS Cloud9 Console. Select My environments. Click Create environment. Download the Kinesis Client Library examples found here and unzip them on your local machine.\nIn Cloud9 environment.\nSelect File.\nSelect Upload Local Files.\nSelect Select folder and upload file that we downloaded.\nOnce the code has successfully been uploaded, let\u0026rsquo;s open up the following directory and we will look like this:\nWe will run the command cd kinesis-kcl-example/kcl-app to navigate to the kcl-app. We run the below command to install and setup environment to run code. sudo yum install maven -y mvn clean compile assembly\\:single Next, we will run the below command to create some Environment variables. export STREAM_NAME=\u0026lt;your-kinesis-stream\u0026gt; export AWS_REGION=\u0026lt;your-aws-region\u0026gt; export APPLICATION_NAME=ImmersiondayKCLConsumer To comsume data to the Kinesis Data Stream, we will run the following command: java -jar target/kcl-app-1.0-SNAPSHOT-jar-with-dependencies.jar. To write data to Kinesis Data Stream, we can use the python CDK function in Cloud9 Environment. Open new terminal. Run the command python3 kds-py-sdk.py to start write data into. Switching to the DynamoDB Console you will see a table named ImmersiondayKCLConsumer which is the lease table for the KCL application.\nWhen you are running KCL, it will by default publish metrics to Amazon Cloudwatch. Navigate To Cloudwatch Metrics and select the ImmersiondayKCLConsumer namespace. You will see KCL-Application, Worker and Shard metrics.\n"
},
{
	"uri": "/5-consumerwithkdf/5.2-createdb/",
	"title": "Create AWS Glue and AWS Athena",
	"tags": [],
	"description": "",
	"content": "AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analytics. It automates the difficult and time-consuming tasks of data discovery, conversion, mapping, and job scheduling.\nAWS Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. It allows you to run ad-hoc queries on large datasets stored in S3 without the need for complex ETL jobs or data warehouse infrastructure.\nAWS Glue and AWS Athena are often used together in data lake architectures. AWS Glue can be used to prepare and transform data, load it into Amazon S3, and catalog it in the AWS Glue Data Catalog. Once the data is in S3 and cataloged in the Glue Data Catalog, you can use AWS Athena to run ad-hoc SQL queries on the data, enabling data analysts and data scientists to gain insights without the need for specialized data warehousing infrastructure.\nIn this step, we need to create a AWS Glue database and AWS Athena.\nWe will use the kinesis-glue-db database that we created in the previous step. Create Athena. Go to the AWS Athena Console. Select Query editor. Select Settings. Click Manage. In Manage settings. Select S3 bucket, Example: s3://kinesis-data-firehose-lab/nyctaxitrips. Click Save. After that, we will navigate to the Editor. Select Editor. At Database, choose kinesis-glue-db. Copy and paste the below sql command. CREATE EXTERNAL TABLE `nyctaxitrips` ( `id` string, `vendorId` int, `pickupDate` string, `dropoffDate` string, `passengerCount` int, `pickupLongitude` double, `pickupLatitude` double, `dropoffLongitude` double, `dropoffLatitude` double, `storeAndFwdFlag` string, `gcDistance` double, `tripDuration` int, `googleDistance`int, `googleDuration`int, `source`string ) PARTITIONED BY ( `year` string, `month` string, `day` string, `hour` string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; LOCATION \u0026#39;s3://\u0026lt;\u0026lt;BUCKET-NAME\u0026gt;\u0026gt;/nyctaxitrips/\u0026#39; Click Run.\nThe result will look like this:\nAfter executing the command, a table will be created in the kinesis-glue-db database. Go to the kinesis-glue-db database. It will look like this: "
},
{
	"uri": "/2-prerequiste/2.2-createkinesisdatastream/",
	"title": "Create Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website click streams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\nIn this step, we need to create a Kenisis Data Stream.\nCreate Kinesis Data Stream Go to the AWS Kinesis Console. Select Data streams. Click Create data stream. In Create data stream. Enter kinesis-stream. Scroll down and click Create data stream. "
},
{
	"uri": "/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "Overview AWS Cloud9 is an integrated development environment, or IDE. The AWS Cloud9 IDE offers a rich code-editing experience with support for several programming languages and runtime debuggers, and a built-in terminal. It contains a collection of tools that you use to code, build, run, test, and debug software, and helps you release software to the cloud.\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website click streams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\nAWS Identity and Access Management (IAM) roles are entities you create and assign specific permissions to that allow trusted identities such as workforce identities and applications to perform actions in AWS.\nAmazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. With cost-effective storage classes and easy-to-use management features, you can optimize costs, organize data, and configure fine-tuned access controls to meet specific business, organizational, and compliance requirements.\nAmazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don\u0026rsquo;t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. DynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.\nContent Create Cloud9 Environment Create Kinesis Data Stream Create IAM Roles Create S3 Buckets Create DynamoDB Table "
},
{
	"uri": "/3-writedataintokds/3.3-producerwithstudionotebook/3.3.2-runstudionotebook/",
	"title": "Run Studio notebook",
	"tags": [],
	"description": "",
	"content": "In this step, we will perform these actions:\nQuery data from S3. Create an in-memory table for target kinesis stream. Start data insertion from S3 into Kinesis Stream using in-memory tables. Query Kinesis stream target table to check data is inserted correctly. Upload file to read and load data into Kinesis Stream. In the Apache Zeppelin Console. Select Import note. Choose Select JSON File/IPYNB File and upload this file to to read and load data into Kinesis Stream. Now we can click Taxi Trips Data Loading from S3 to Kinesis-1 to start. Load Taxi Trips Data from S3 bucket into Kinesis stream. In Taxi Trips Data Loading from S3 to Kinesis-1. You need to replace with your S3 bucket. After that, click button play to create nyc_yellow_taxi_trip_data table in AWS Glue Database. After performing this action, a nyc_yellow_taxi_trip_data table will be created in the AWS Glue Database.\nNext, we will run Query data from S3. The result will look like this: Next, we need to create an in-memory table for target kinesis stream. Next, we will start data insertion from S3 into Kinesis Stream using in-memory tables and execute query. Now, we have completed using the Studio notebook to read data from S3 and write it to the Kinesis Data Stream throught the AWS Glue Database.\nTo clean up, we will follow other steps Taxi Trips Data Loading from S3 to Kinesis-1.\n"
},
{
	"uri": "/4-consumedatainkds/4.3-consumerwithamazonmdf/4.3.2-createstudionotebook/",
	"title": "Tạo Studio Notebook",
	"tags": [],
	"description": "",
	"content": "Amazon Managed Service for Apache Flink (shortened as Amazon MSK for Apache Flink or simply MSK for Flink) is a fully managed service provided by Amazon Web Services (AWS) that simplifies the process of running Apache Flink applications on the AWS cloud. Apache Flink is an open-source stream processing framework that can be used for real-time analytics, event-driven applications, and more.\nWith Amazon Managed Service for Apache Flink, you can use Java, Scala, Python, or SQL to process and analyze streaming data. The service enables you to author and run code against streaming sources and static sources to perform time-series analytics, feed real-time dashboards, and metrics.\nIn this step, we need to create a Studio Notebook to start write data from Kinesis Data Stream to Amazon OpenSearch Service.\nCreate Studio notebooks. Go to the AWS Kinesis Console. Select Managed Apache Flink. In Managed Apache Flink. Select Studio notebooks. Click Create Studio notebook. In Create Studio notebook. Select Create with custom settings. Enter kinesis-studio-mdf. Scroll down and click Next. In IAM permissions step. Select Choose from IAM roles that Managed Service for Apache Flink can assume. At Service role, select kinesis-studio-lab-role. At AWS Glue database, select kinesis-glue-db. Click Next. At Custom connectors. Click Add custom connector. Select kinesis-amazon-msk-flink-lab. At Path to S3 object, enter flink-sql-connector/flink-sql-connector-elasticsearch7_2.12-1.13.2.jar. Click Save changes. After that, we select Next and Create Studio notebook. After creating succesfully, we will select kinesis-studio-mdf. Click Run to start the Studio Notebook. To access to the Studio Notebook, we will click Open in Apache Zeppelin. "
},
{
	"uri": "/3-writedataintokds/3.2-producerwithkpl/",
	"title": "Write data with Kinesis Producer Client",
	"tags": [],
	"description": "",
	"content": "The Kinesis Producer Library (KPL) enhances the data ingestion capabilities covered in the AWS SDK Kinesis API section of this workshop, allowing for developers to achieve higher and more optimal write throughput with retries and error handling, utilizing shards within the data stream to their maximum capacity per second.\nIn this step, we\u0026rsquo;ll use the Kinesis Producer Library (KPL) to write data into the Kinesis Data Stream.\nCreate Producer code Go to the AWS Cloud9 Console. Select My environments. Click Create environment. Download the Kinesis Producer Library examples found here and unzip them on your local machine.\nIn Cloud9 environment.\nSelect File.\nSelect Upload Local Files.\nSelect Select folder and upload file that we downloaded.\nOnce the code has successfully been uploaded, let\u0026rsquo;s open up the following directory.\nRun the command cd kinesis-producer-library-examples-master/.\nRun the command mvn clean compile package. This will compile and package a jar containing all code within the project we are working with. The jar will be generated and placed in the target/ folder of your project workspace.\nWe will run the command java -cp target/amazon-kinesis-replay-1.0-SNAPSHOT.jar A_SimpleProducer to execute java function to write data into Kinesis Data Stream and press Ctrl C to stop.\nWait a couple of minus, and navigate to Kinesis Data Stream. Select Monitor. We can monitor data was put into Kinesis in Incoming data and PutRecords. "
},
{
	"uri": "/4-consumedatainkds/4.3-consumerwithamazonmdf/",
	"title": "Consume data with AWS Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "Overview Amazon Managed Service for Apache Flink (shortened as Amazon MSK for Apache Flink or simply MSK for Flink) is a fully managed service provided by Amazon Web Services (AWS) that simplifies the process of running Apache Flink applications on the AWS cloud. Apache Flink is an open-source stream processing framework that can be used for real-time analytics, event-driven applications, and more.\nWith Amazon Managed Service for Apache Flink, you can use Java, Scala, Python, or SQL to process and analyze streaming data. The service enables you to author and run code against streaming sources and static sources to perform time-series analytics, feed real-time dashboards, and metrics.\nIn this step, we will consume data with Amazon Managed Service for Apache Flink.\nContent Create OpenSearch Service Create Studio Notebook Stream data and validate output "
},
{
	"uri": "/2-prerequiste/2.3-createiamrole/",
	"title": "Create IAM Roles",
	"tags": [],
	"description": "",
	"content": "AWS Identity and Access Management (IAM) Roles are entities you create and assign specific permissions to that allow trusted identities such as workforce identities and applications to perform actions in AWS. When your trusted identities assume IAM roles, they are granted only the permissions scoped by those IAM roles.\nIn this step, we need to create IAM Roles to grant permission for this lab.\nCreate IAM Policy Go to the AWS IAM Console. Select Policies. Click Create policy. In Specify permission step. Select Json. Coppy and paste the below code You need to replace , and the corresponding service name.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogGroups\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogStreams\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCloudwatchLogs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/hive\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadConnection\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetConnection\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:connection/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:DeleteTable\u0026#34;, \u0026#34;glue:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:table/kinesis-glue-db/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VPCReadOnlyPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeDhcpOptions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadWriteCode\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-analytics-placeholder-s3-bucket/kinesis-analytics-placeholder-s3-path/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:ResourceAccount\u0026#34;: \u0026#34;\u0026lt;your-aws-account-id\u0026gt;\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;DescribeApplication\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesisanalytics:DescribeApplication\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesisanalytics:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:application/kinesis-notebook-*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadCustomArtifact\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-studio-notebook-lab\u0026#34;, \u0026#34;arn:aws:s3:::kinesis-studio-notebook-lab/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetUserDefinedFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:userDefinedFunction/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesis:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next. In Review and create step. Enter kinesis-studio-lap-policy. Scroll down and click Create. Create IAM Role Go to the AWS IAM Console. Select Roles. Click Create role. In Specify trusted entity step. Choose AWS service. At service or usecase, choose Kinesis. At usecase, choose Kinesis Analystics. In Add permissions step. Search and select kinesis-studio-lap-policy policy. Click Next. In Review and create step. Enter kinesis-studio-lap-role. Scroll down and click Create. Follow these above steps, we need to create an IAM Role with name kinesis-amazon-msk-flink-lab-role for comsuming data with Amazon Managed Service for Apache Flink.\nThe policy will like this:\nYou need to replace , and the corresponding service name.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogGroups\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogStreams\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCloudwatchLogs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/hive\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadConnection\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetConnection\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:connection/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:DeleteTable\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:table/kinesis-glue-db/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VPCReadOnlyPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeDhcpOptions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadWriteCode\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-analytics-placeholder-s3-bucket/kinesis-analytics-placeholder-s3-path/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:ResourceAccount\u0026#34;: \u0026#34;\u0026lt;your-aws-account-id\u0026gt;\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;DescribeApplication\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesisanalytics:DescribeApplication\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesisanalytics:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:application/kinesis-studio-mdf\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadCustomArtifact\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-amazon-msk-flink-lab/flink-sql-connector/flink-sql-connector-elasticsearch7_2.12-1.13.2.jar\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetUserDefinedFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:userDefinedFunction/*\u0026#34; ] } ] } Create IAM Role for Lambda Function We alse need to create a IAM Role with kinesis-lambda-lab-role for Lambda function can integrate with Kinesis to consume data. The policy will like this: You need to replace , and the corresponding service name.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:UpdateTable\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:table/kinesisAggs\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:ListContributorInsights\u0026#34;, \u0026#34;dynamodb:ListGlobalTables\u0026#34;, \u0026#34;dynamodb:ListTables\u0026#34;, \u0026#34;dynamodb:ListBackups\u0026#34;, \u0026#34;dynamodb:ListExports\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesis:GetRecords\u0026#34;, \u0026#34;kinesis:GetShardIterator\u0026#34;, \u0026#34;kinesis:DescribeStream\u0026#34;, \u0026#34;kinesis:DescribeStreamSummary\u0026#34;, \u0026#34;kinesis:ListShards\u0026#34;, \u0026#34;kinesis:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesis:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:stream/kinesis-stream\u0026#34; } ] } "
},
{
	"uri": "/5-consumerwithkdf/5.3-createkdf/",
	"title": "Create Kinesis Data Firehose",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\nIn this step, we will create a Kinesis Data Firehose to consume data from Kinesis Data Stream.\nCreate Kinesis Data Firehose. Go to the AWS Kinesis Console. Select Amazon Data Firehose. In Amazon Data Firehose console. Click Create Firehose stream. In Create Firehose stream. At Source, select Amazon Kinesis Data Stream. Select kinesis-stream. At Transform and convert records. Enable Turn on data transformation. Select dataTransformHandler function. Enable Convert record format. Select Apache Parquet. At AWS Glue region, select your AWS Glue region. Select kinesis-glue-db. Select nyctaxitrips. At Destination settings. Select kinesis-data-firehose-lab bucket. At S3 bucket prefix, copy and paste the following: nyctaxitrips/year=!{timestamp:YYYY}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/. At S3 bucket error output prefix, copy and paste the following: nyctaxitripserror/!{firehose:error-output-type}/year=!{timestamp:YYYY}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/. At Buffer hints, compression, file extension and encryption. Adjusting the Buffer interval down 60 second. Scroll down and click Create Firehose stream. "
},
{
	"uri": "/4-consumedatainkds/4.3-consumerwithamazonmdf/4.3.3-streamdata/",
	"title": "Stream data and validate output",
	"tags": [],
	"description": "",
	"content": "In this step, we will start streaming data and validate output.\nWe need download the Zeppelin Notebook File, which we\u0026rsquo;ll use to read and load data into OpenSearch.\nStarting Stream data. In the previous step, we access to the Studio Notebook throuhout Apache Zeppelin. In Apache Zeppelin. Select Import note. Select JSON File/IPYNB File and upload the file you just downloaded. Start consume data from Kinesis Data Stream and load to OpenSearch Service. Click KDA-OpenSearch. In KDA-OpenSearch. We will create a taxi_trips table to hold Kinesis Stream Data, we need click Run button. Make sure you have chosen the correct aws_region and aws_kinesis_name.\nNext, we will start consume data from Kinesis Data Stream. Now, we need to write data to the Kinesis Data Stream, we can write data with python CDK function in Cloud9 Environment. After wrote data to the Kinesis Data Stream, we will see like this: Next, we need to create a Taxi Statistics Table to hold aggregated data in Amazon OpenSearch. Click Run button to start create table. Make sure you have chosen the correct opensearch_service_hosts (Open Search Domain endpoint v2), and the username and password are for the admin user that you created when setting up the OpenSearch Service Domain.\nNow, we wil run the following cell to aggregate taxi data from taxi_trip table and save aggregated data in OpenSearch Service. Validate Output Go to the Amazon OpenSearch Service Console. Select Domains. Select kinesis-opensearch-service and open OpenSearch Dashboards URL. We will use the admin and password are for the admin user that you created when setting up the OpenSearch Service Domain.\nIn OpenSearch Dashboards. Select Menu button. Select Query Workbench. In Query Workbench Copy and paste the SQL command select * from trip_statistics;. Select Run. We will see like this: After that, we will navigate to the Dashboards Management. Select Index patterns. Click Create index patterns. In Define an index pattern step. Enter trip_statistics. Click Next step. In the next step, we will click Create pattern. Next, we will navigate to the Visualize. Click Create new visualization. Select Gauge and after that select trip_statistics. Create a visualization: "
},
{
	"uri": "/3-writedataintokds/",
	"title": "Write data to Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Overview AWS Kinesis SDK is a software development kits (SDKs) provided by Amazon Web Services (AWS) to interact with Amazon Kinesis services.\nThe Kinesis Producer Library (KPL) enhances the data ingestion capabilities covered in the AWS SDK Kinesis API section of this workshop, allowing for developers to achieve higher and more optimal write throughput with retries and error handling, utilizing shards within the data stream to their maximum capacity per second.\nStudio notebooks for Managed Service for Apache Flink allows you to interactively query data streams in real time, and easily build and run stream processing applications using standard SQL, Python, and Scala. With a few clicks in the AWS Management console, you can launch a serverless notebook to query data streams and get results in seconds.\nIn this step, we will perform write data to the Kinesis Data Stream.\nContent Write data with SDK Write data with Kinesis Producer Library Write data with Studio Notebooks "
},
{
	"uri": "/3-writedataintokds/3.3-producerwithstudionotebook/",
	"title": "Write data with Studio notebooks",
	"tags": [],
	"description": "",
	"content": "Overview Studio notebooks for Managed Service for Apache Flink allows you to interactively query data streams in real time, and easily build and run stream processing applications using standard SQL, Python, and Scala. With a few clicks in the AWS Management console, you can launch a serverless notebook to query data streams and get results in seconds.\nA notebook is a web-based development environment. With notebooks, you get a simple interactive development experience combined with the advanced capabilities provided by Apache Flink. Studio notebooks uses notebooks powered by Apache Zeppelin, and uses Apache Flink as the stream processing engine. Studio notebooks seamlessly combines these technologies to make advanced analytics on data streams accessible to developers of all skill sets.\nApache Zeppelin provides your Studio notebooks with a complete suite of analytics tools, including the following:\nData Visualization. Exporting data to files. Controlling the output format for easier analysis. In this step, we\u0026rsquo;ll use the Studio notebooks for Managed Service for Apache Flink to write data into the Kinesis Data Stream.\nContent Create Studio Notebook Run Studio Notebook "
},
{
	"uri": "/4-consumedatainkds/",
	"title": "Consume from Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Overview AWS Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, and logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.\nKinesis Client Library for data analysis, archival, real-time dashboards, and much more. While you can use Amazon Kinesis API functions to process stream data directly, the KCL takes care of many complex tasks associated with distributed processing and allows you to focus on the record processing logic. For example, the KCL can automatically load balance record processing across many instances, allow the user to checkpoint records that are already processed, and handle instance failures. The KCL acts as an intermediary between your record processing logic and Kinesis Data Streams. The KCL performs the following tasks.\nAmazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\nAmazon Managed Service for Apache Flink (shortened as Amazon MSK for Apache Flink or simply MSK for Flink) is a fully managed service provided by Amazon Web Services (AWS) that simplifies the process of running Apache Flink applications on the AWS cloud. Apache Flink is an open-source stream processing framework that can be used for real-time analytics, event-driven applications, and more.\nWith Amazon Managed Service for Apache Flink, you can use Java, Scala, Python, or SQL to process and analyze streaming data. The service enables you to author and run code against streaming sources and static sources to perform time-series analytics, feed real-time dashboards, and metrics.\nIn this step, we will perform consume data from the Kinesis Data Stream.\nContent: Consume data with Lambda Consume data with Kinesis Client Library Consume data with Amazon Managed Service for Apache Flink "
},
{
	"uri": "/2-prerequiste/2.4-creates3bucket/",
	"title": "Create S3 Buckets",
	"tags": [],
	"description": "",
	"content": "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.\nIn this step, we need to create S3 buckets to store the data required for this lab.\nCreate S3 buckets for Studio Notebook Go to the AWS S3 Console. Select Buckets. Click Create bucket. In Create bucket. Enter kinesis-studio-notebook-lab. Scroll down and click Create bucket. Next, let\u0026rsquo;s download the Taxi Ride Data file which we\u0026rsquo;ll use to load into Kinesis Stream.\nUpload to kinesis-studio-notebook-lab bucket the file that we just downloaded.\nNavigate into kinesis-studio-notebook-lab.\nClick Upload.\nIn Upload. Upload file we just downloaded. Click Upload. Create S3 buckets for Amazon Managed Service for Apache Flink Go to the AWS S3 Console. Select Buckets. Click Create bucket. In Create bucket. Enter kinesis-amazon-msk-flink-lab. Scroll down and click Create bucket. In kinesis-amazon-msk-flink-lab. Create flink-sql-connector. Download and upload Flink-sql-connector-elasticsearch. Create S3 buckets for Kinesis Data Firehose Following the above steps, we will create a bucket with name kinesis-data-firehose-lab or any other names. We also need to create a folder nyctaxitrips/. "
},
{
	"uri": "/5-consumerwithkdf/5.4-streamdata/",
	"title": "Stream data and validate output",
	"tags": [],
	"description": "",
	"content": "In this step, we will Stream the data and validate output.\nWrite data to Kinesis Data Stream. Access into Cloud9 Environment. We will use python CDK to write data to the Kinesis Data Stream. If you don\u0026rsquo;t have python CDK function, download the code here and upload to the Cloud9 Environment. We will run the command pip install boto3 to install boto3 library. After that, we need run the command python3 kds-py-sdk.py to write data to the Kinesis Data Stream. The result will look like this: Validate output. Navigate to the Kinesis Console and click on the “nyc-taxi-trips” Data Firehose delivery stream. We will see like this in Monitor. Go to the kinesis-data-firehose-lab bucket to see where the data is put. Select one of the parquet files. Click Action. Select Query with S3 Select. In Query with S3 Select. At Format, select Apache Parquet. At Output settings, select CSV and Comma. Next, we will click Run SQL query. The result will look like: After that, we will navigate to the Athena Query editor. At nyctaxitrips talbe, click vertical ellipsis. Select Load partitions. It will create and execute a new Query.\nAfter that, we will create new Query. At nyctaxitrips talbe, click vertical ellipsis. Select Preview Table. It will also create and execute a new Query.\nThe result will look like this: "
},
{
	"uri": "/5-consumerwithkdf/",
	"title": "Consume data with Kinesis Data Firehose",
	"tags": [],
	"description": "",
	"content": "Overview Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\nIn this step, we will use Amazon Kinesis Data Firehose to consume data from Amazon Kinesis Data Stream, we will use AWS Lambda to tranform data before send it to AWS Glue and AWS Athena.\nContent Create Lambda function Create Glue database and Athena Create Kinesis Data Firehose Stream data and validate output "
},
{
	"uri": "/2-prerequiste/2.5-createdynamodb/",
	"title": "Create DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don\u0026rsquo;t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. DynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.\nIn this step, we will create a DynamoDB table to store data from Kinesis.\nCreate DynamoDB Table Go to the AWS DynamoDB Console. Select Tables. Click Create table. In Create table. Enter kinesisAggs. At Partition key, enter vendorId. Scroll down and click Create table. When creating a Table and defining your DynamoDB schema, one of the first options you’ll be asked is to specify your Table’s Partition Key and Sort Key. This is an important decision that has impact on how your table’s record’s can be accessed.\nA DynamoDB Partition Key serves as the primary identifier for partitioning your data across DynamoDB\u0026rsquo;s multiple storage nodes. It\u0026rsquo;s a mandatory component when setting up a DynamoDB table. The Partition Key helps distribute your data across different partitions.\nA DynamoDB Sorted Key is an optional attribute that allows for the sorting of items within each partition. By specifying a Sort Key, you enable DynamoDB to order the records within a partition based on the Sort Key\u0026rsquo;s value.\n"
},
{
	"uri": "/6-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete Kinesis Data Stream Go the the AWS Kinesis Console. Select Data streams. Choose kinesis-stream. Select Actions. Select Delete. Delete Studio Notebook Go to the AWS Managed Apache Flink Console. Select Studio notebooks. Choose kinesis-studio-mdf. Select Actions. Select Delete. Delete AWS Glue Database Go to the AWS Glue Console. Select Databases. Choose kinesis-glue-db. Select Delete. Delete DynamoDB Go to the AWS DynamoDB Console. Select Tables. Choose kinesisAggs and ImmersiondayKCLConsumer. Click Delete. Delete S3 Buckets Go to the AWS S3 Console. Select Buckets. Choose kinesis-data-firehose-lab. Before delete bucket, we need to empty it.\nSelect Empty Enter permanently delete. After that, we will delete kinesis-data-firehose-lab bucket, select kinesis-data-firehose-lab and click Delete. We will do the same for kinesis-amazon-msk-flink-lab and kinesis-studio-notebook-lab.\nDelete IAM Roles and IAM Policies Go to the AWS IAM Console Select Roles Search and select kinesis-studio-lab-role Click Delete After that, we will select Policies. Search and select kinesis-studio-lab-policy. Click Delete. We will do the same for kinesis-lambda-lab-role and kinesis-amazon-msk-flink-lab-role.\nDelete OpenSearch Service Go to the AWS OpenSearch Service Console. Select Domains. Choose kinesis-opensearch-service. Click Delete. "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]