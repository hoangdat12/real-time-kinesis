[
{
	"uri": "/vi/",
	"title": "Amazon Kinesis",
	"tags": [],
	"description": "",
	"content": "Truyền dữ liệu thời gian thực với Amazon Kinesis Tổng quan In this lab, you\u0026rsquo;ll learn the basics and practice streaming data with Amazon Kinesis. Perform producing and consuming data to understand the end-to-end process of real-time data ingestion, processing, and analysis using Amazon Kinesis streams and its associated services.\nNội dung Giới thiệu Các bước chuẩn bị Producing dữ liêu Consuming dữ liêu Consuming dữ liêu với Kinesis Data Firehose Dọn dẹp tài nguyên "
},
{
	"uri": "/vi/4-consumedatainkds/4.1-consumerwithlambda/",
	"title": "Consume dữ liệu với Aws Lambda",
	"tags": [],
	"description": "",
	"content": "AWS Lambda là một dịch vụ tính toán cho phép bạn chạy mã mà không cần cung cấp hoặc quản lý máy chủ. Lambda chạy code của bạn trên một cơ sở hạ tầng tính toán có khả năng cao và thực hiện tất cả các công việc quản lý tài nguyên tính toán, bao gồm bảo trì máy chủ và hệ điều hành, cung cấp và tự động mở rộng dung lượng, và ghi log. Với Lambda, tất cả những gì bạn cần làm là cung cấp mã của mình trong một trong các thời gian chạy ngôn ngữ mà Lambda hỗ trợ.\nTrong bước này, chúng ta sẽ sử dụng AWS Lambda để tiêu thụ dữ liệu từ Kinesis. Chúng ta cần tạo một hàm Lambda và thêm một kích hoạt từ Kinesis.\nTạo Lambda function. Truy cập vào Giao diện quản lý AWS Lambda. Chọn Functions. Click Create function. Trong Create function. Nhập kinesisConsumeData. Tại Runtime, chọn python 3.12 (phiên bản mới nhất). Tại Change default execution role, chọn Use a existing role. Tìm kiếm và chọn kinesis-lambda-lab-role. Kéo xuống và click Create function. Trong kinesisConsumeData function. Click Upload from và chọn .zip file. Tải zip file này có Lambda Consumer code và các gói cần thiết vào nó tải về máy của bạn và tải lên Lambda Function. Chúng ta cần phải thêm một biến môi trường là dynamoDBTableName với giá trị là tên của DynamoDB table mà bạn đã tạo.\nThêm Kinesis Trigger. Truy cập vào Lambda function. Chọn Add Trigger. Trong Add trigger. Chọn Kinesis. Chọn kinesis-stream. Bây giờ, chúng ta sẽ cấu hình như sau: Viết dữ liệu vào Kinesis Data Stream. Truy cập vào Cloud9 Environment. Chúng ta sẽ sử dụng python CDK để viết dữ liệu vào Kinesis Data Stream. Tải code ở đây và upload lên Cloud9 Environment. Chúng ta sẽ thực thi câu lệnh pip install boto3 để cài đặt thư viện boto3. Sau đó, chúng ta cần thực thi câu lệnh python3 kds-py-sdk.py để bắt đầu viết dữ liệu vào Kinesis Data Stream. Kết quả sẽ trong như thế này: Bây giờ, chúng ta sẽ truy cập vào CloudWatch logs để xem những gì đã xảy ra khi chúng ta viết dữ liệu vào Kinesis Data Stream. Truy cập vào Lambda function. Chọn Monotor. Click View in CloudWatch logs. Trong CloudWatch logs, chúng ta sẽ thầy như thế này: Truy cập vào Giao diện quản lý AWS DynamoDB. Chọn Explore items. Chọn kinesisAggs. "
},
{
	"uri": "/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Streams (KDS) là một dịch vụ truyền dữ liệu thời gian thực có khả năng mở rộng lớn và bền vững. KDS có thể liên tục thu thập hàng tỷ byte dữ liệu mỗi giây từ hàng trăm nghìn nguồn như luồng click trên trang web, luồng sự kiện cơ sở dữ liệu, giao dịch tài chính, luồng thông tin trên mạng xã hội, logs IT và sự kiện theo dõi vị trí. Dữ liệu thu thập được sẵn sàng trong vài mili giây để hỗ trợ các ứng dụng phân tích thời gian thực như bảng điều khiển thời gian thực, phát hiện bất thường thời gian thực, định giá động và nhiều ứng dụng khác.\nBạn có thể sử dụng Amazon Kinesis Data Streams để nhập lượng lớn dữ liệu trong thời gian thực, lưu trữ dữ liệu một cách bền vững và làm cho dữ liệu có sẵn để tiêu thụ. Chúng ta có thể học về kiến thức cơ bản của KDS:\nĐơn vị dữ liệu được lưu trữ bởi Kinesis Data Streams là một Record dữ liệu.\nCác bản ghi dữ liệu trong một luồng dữ liệu được phân phối vào Shards. Một shard chứa một chuỗi bản ghi dữ liệu trong một luồng. Khi bạn tạo một luồng, bạn chỉ định số lượng shard cho luồng đó. Tổng dung lượng của một luồng là tổng dung lượng của các shard của nó. Bạn có thể tăng hoặc giảm số lượng shard trong một luồng khi cần thiết. Tuy nhiên, bạn sẽ bị tính phí dựa trên mỗi shard. Để biết thông tin về dung lượng và giới hạn của một shard, xem Kinesis Data Streams Limits.\nCác ứng dụng hoặc hệ thống tạo ra và gửi dữ liệu đến Kinesis Data Streams được gọi là Producers. Các Producers có thể bao gồm Amazon Kinesis Data Streams API, Amazon Kinesis Producer Library (KPL), Amazon Kinesis Agent — một ứng dụng Java đã được xây dựng trước dành cho việc thu thập và truyền dữ liệu dễ dàng đến luồng Amazon Kinesis của bạn, Lambda, Amazon DynamoDB, Amazon Aurora, Amazon CloudWatch, AWS IoT Core và nhiều hơn nữa.\nCác ứng dụng hoặc hệ thống lấy dữ liệu từ Kinesis Data Streams để xử lý, phân tích hoặc lưu trữ được gọi là Consumers. Các Consumers có thể bao gồm AWS Lambda, Amazon Managed Service Apache Flink do Amazon quản lý, AWS Glue Streaming, Amazon Kinesis Client Library (KCL) - thư viện đã được xây dựng trước giúp bạn dễ dàng xây dựng ứng dụng Amazon Kinesis để đọc và xử lý dữ liệu từ một luồng dữ liệu Amazon Kinesis, Kinesis Data Firehose và nhiều hơn nữa.\nKinesis Data Firehose có thể gửi bản ghi dữ liệu đến các đích đến khác nhau, bao gồm Dịch vụ Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, DynamoDB và bất kỳ điểm cuối HTTP nào thuộc sở hữu của bạn hoặc bất kỳ nhà cung cấp dịch vụ bên thứ ba nào của bạn.\nKiến trúc cơ bản của Amazon Kinesis Data Stream sẽ như bên dưới:\nTrong bài lab này, bạn sẽ được học cơ bản và thực hành về truyền dữ liệu thời gian thực với Amazon Kinesis. Chúng ta sẽ đi sâu vào các chi tiết của việc thiết lập Kinesis Data Stream (KDS), hiểu về ý nghĩa của Shards, và cấu hình các thuộc tính của luồng. Bạn sẽ khám phá các phương pháp khác nhau để đưa dữ liệu vào KDS một cách hiệu quả, tận dụng các công cụ như AWS SDK và AWS CLI. Thêm vào đó, chúng ta sẽ khám phác các điều kiện cần thiết để có thể tiêu thụ dữ liệu từ KDS, triển khai các consumer mạnh mẽ và đảm bảo xử lý dữ liệu mượt mà. Đến cuối bài lab này, bạn có thể hiểu tổng quan về Amazon Kinesis và vai trò quan trọng của nó trong việc xây dựng ứng dụng truyền dữ liệu thời gian thực có khả năng mở rộng và khả năng thay đổi nhanh chóng.\n"
},
{
	"uri": "/vi/2-prerequiste/2.1-createcloud9/",
	"title": "Tạo Cloud9 Environment",
	"tags": [],
	"description": "",
	"content": "AWS cloud9 là một môi trường phát triển tích hợp, hoặc IDE. AWS Cloud9 IDE cung cấp một trải nghiệm viết code phong phú với hỗ trợ cho nhiều ngôn ngữ lập trình và trình gỡ lỗi , cùng với một terminal tích hợp sẵn. Nó chứa một tập hợp các công cụ giúp bạn viết code, xây dựng, chạy, kiểm tra và gỡ lỗi phần mềm, nó giúp bạn triển khai phầm mềm trên cloud.\nTrong bước này, chúng ta cần cấu hình một Cloud9 Environment để chạy Python và Javascrip code, cùng với đó là Kinesis Producer Library và Kinesis Client Library.\nTạo Cloud9 Environment Truy cập vào AWS Cloud9 Console. Chọn My environments. Click Create environment. Trong Create environment. Nhập kinesis-env. Chọn New EC2 instance. Tại New EC2 instance, chọn t2.micro. Kéo xuống và click Create. Sau khi hoàn tất việc tạo AWS Cloud9 Environment, click Open để truy cập vào Cloud9 environment. Chạy lệnh sudo yum install maven -y để cài đặt Maven.\n"
},
{
	"uri": "/vi/5-consumerwithkdf/5.1-createlambda/",
	"title": "Tạo Lambda function",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta cần tạo một Lambda function để chuyển đổi dữ liệu mà Kinesis Data Firehose nhận được từ Kinesis Data Stream.\nTạo Kinesis Data Firehose. Truy cập vào Giao diện quản lý AWS Lambda Chọn Functions. Click Create function. Trong Create function. Nhập dataTransformHandler. Tại Runtime, chon python 3.12 (phiên bản mới nhất). Click Next. Sau khi tạo thành công, chúng ta sẽ dán đoạn code bên dưới vào Lambda function mà chúng ta vừa tạo: import base64 import json print(\u0026#39;Loading function\u0026#39;) def lambda_handler(event, context): output = [] for record in event[\u0026#39;records\u0026#39;]: print(record[\u0026#39;recordId\u0026#39;]) # Decode the base64 encoded data and convert to string payload = base64.b64decode(record[\u0026#39;data\u0026#39;]).decode(\u0026#39;utf-8\u0026#39;) # Convert the string payload to a JSON object reading = json.loads(payload) # Add additional column \u0026#39;source\u0026#39; reading[\u0026#39;source\u0026#39;] = \u0026#39;NYCTAXI\u0026#39; # Encode the modified JSON object to base64 output_data = base64.b64encode(json.dumps(reading).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;) # Prepare the output record output_record = { \u0026#39;recordId\u0026#39;: record[\u0026#39;recordId\u0026#39;], \u0026#39;result\u0026#39;: \u0026#39;Ok\u0026#39;, \u0026#39;data\u0026#39;: output_data } output.append(output_record) print(\u0026#39;Successfully processed {} records.\u0026#39;.format(len(event[\u0026#39;records\u0026#39;]))) return {\u0026#39;records\u0026#39;: output} Sau đó chúng ta cần chỉnh sửa timout của Lambda function từ 3s lên 1 phút. Trong Lambda function. Chọn Configuration. Chọn General configuration. Click Edit. Trong Edit basic settings. Tại Timeout, điền 1. "
},
{
	"uri": "/vi/4-consumedatainkds/4.3-consumerwithamazonmdf/4.3.1-createopensearchservice/",
	"title": "Tạo OpenSearch Service",
	"tags": [],
	"description": "",
	"content": "OpenSearch là một công cụ tìm kiếm và phân tích hoàn toàn mã nguồn mở dành cho các trường hợp sử dụng như phân tích log, giám sát ứng dụng thời gian thực và clickstream analysis.\nAmazon OpenSearch Service cung cấp tất cả các tài nguyên cho cụm OpenSearch của bạn và khởi động nó. Nó cũng tự động phát hiện và thay thế các nút dịch vụ OpenSearch bị lỗi, giảm thiểu các chi phí liên quan đến việc quản lý hạ tầng tự quản lý. Bạn có thể mở rộng cụm của mình bằng một cuộc gọi API đơn hoặc vài cú nhấp chuột trên bảng điều khiển.\nTrong bước này, chúng ta cần tạo một OpenSearch Service để lưu trữ dữ liệu từ Amazon Managed Service for Apache Flink nhận được từ Kinesis Data Stream để tiến hành phân tích.\nTạo OpenSearch Service. Truy cập vào Giao diện quản lý AWS OpenSearch Service. Chọn Dashboard. Chọn Create domain. Tại Templates, chọn Dev/test. Bởi vì trong bài lab này, chúng ta chỉ cần xử lý một lượng dữ liệu nhỏ, nên không cần tạo production environment. Chúng ta chỉ cần một môi trường phát Dev/test để tiết kiệm chi phí.\nTrong Create domain. Nhập kinesis-opensearch-service. Chọn Standard create. Làm theo các bước câu hình như bên dưới: Sau những bước này, chúng ta sẽ click Create để tạo ra một OpenSearch Service Domain. Đợi khoảng 15 phút, chúng ta sẽ thấy kinesis-opensearch-service domain.\n"
},
{
	"uri": "/vi/3-writedataintokds/3.3-producerwithstudionotebook/3.3.1-createstudionotebook/",
	"title": "Tạo Studio Notebook",
	"tags": [],
	"description": "",
	"content": "AWS Glue là một dịch vụ tích hợp dữ liệu serverless (không máy chủ) giúp người dùng phân tích dữ liệu dữ dễ dàng, khám vá, chuẩn bị, chuyển đổi và tích hợp dữ liệu từ nhiều nguồn khác nhau. Bạn có thể sử dụng nó để phân tích, Machine Learning và phát triển ứng dụng. Nó cũng bao gồm các công cụ tăng hiệu xuất và dữ liệu cho việc soạn thảo, chạy công việc và triển khai quy trình làm việc kinh doanh.\nTrong bước này, chúng ta sẽ tạo một Studio notebook và một AWS Glue Database để bắt đầu viết dữ liệu vào Kinesis Data Stream.\nTạo Glue Database Truy cập vào AWS Kinesis Console. Chọn Databases. Nhập kinesis-glue-db. Click Create database. Tạo Studio notebooks. Truy cập vào AWS Kinesis Console. Chọn Managed Apache Flink. Trong Managed Apache Flink. Chọn Studio notebooks. Click Create Studio notebook. Trong Create Studio notebook. Chọn Create with custom settings. Nhập kinesis-studio. Kéo xuống và click Next. Trong bước IAM permissions. Chọn Choose from IAM roles that Managed Service for Apache Flink can assume. Tại Service role, chọn kinesis-studio-lab-role. Tại AWS Glue database, chọn kinesis-glue-db. Click Next. Trong những bước còn lại, chúng ta chọn Next và Create Studio notebook.\nSau khi tạo thành công, chúng ta sẽ click vào Run để khởi động Studio notebook.\nSau khi khởi động thành công, chúng ta sẽ click Open in Apache Zeppelin để truy cập vào Studio notebook thông qua Apache Zeppelin.\nGiao diện của Apache Zeppelin sẽ trông như thế này:\n"
},
{
	"uri": "/vi/3-writedataintokds/3.1-producerwithsdk/",
	"title": "Viết dữ liệu với Kinesis SDK",
	"tags": [],
	"description": "",
	"content": "AWS Kinesis SDK là bộ công cụ phất triển phần mềm (SDKs) được cung cấp bởi Amazon Web Services (AWS) để tương tác với các dịch vụ của Amazon Kinesis\nTrong bước này, chúng ta sẽ sử dụng AWS SDK cho Javascript để viết dữ liệu vào Kinesis Data Stream.\nTạo Javascript file Truy cập vào AWS Cloud9 Console. Chọn My environments. Click Create environment. Trong Cloud9 environment. Chọn File. Chọn New From Templete. Chọn Javascript File. Sao chép và dán đoạn code này vào trong Javascript file const { KinesisClient, PutRecordCommand } = require(\u0026#39;@aws-sdk/client-kinesis\u0026#39;); const REGION = \u0026#39;ap-southeast-1\u0026#39;; const STREAM_NAME = \u0026#39;kinesis-stream\u0026#39;; const config = { region: REGION, }; const client = new KinesisClient(config); const TOTAL_RECORDS = 20; const streamName = STREAM_NAME; const generateData = (i) =\u0026gt; { return { order_id: `ORD${i}`, order_user_id: `USR${i}`, order_items: [ { item_id: `ITEM00${i}`, item_name: `Product Name ${i}`, quantity: i + 1, price: (i + 1) * 10.99, total_price: (i + 1) * 10.99 * (i + 1), }, { item_id: `ITEM00${i + 1}`, item_name: `Product Name ${i + 1}`, quantity: i + 2, price: (i + 2) * 15.5, total_price: (i + 2) * 15.5 * (i + 2), }, ], order_date: \u0026#39;2024-04-17\u0026#39;, order_total_amount: (i + 1) * 10.99 * (i + 1) + (i + 2) * 15.5 * (i + 2), order_status: \u0026#39;Pending\u0026#39;, }; }; for (let i = 0; i \u0026lt; TOTAL_RECORDS; i++) { const jsonData = generateData(i); const data = JSON.stringify(jsonData); const partitionKey = `PartitionKey-${i}`; const input = { StreamName: streamName, Data: Buffer.from(data), PartitionKey: partitionKey, }; const command = new PutRecordCommand(input); (async () =\u0026gt; { try { const response = await client.send(command); console.log(`Record ${i} sent:`, response); } catch (error) { console.error(`Error sending record ${i}:`, error); } })(); } Lưu đoạn code với tên kds-js-sdk.js hoặc với bất kỳ tên nào khác mà bạn muốn. Click Save. Sau đó, chúng ta sẽ chạy câu lệnh bash npm i @aws-sdk/client-kinesis để cài đặt AWS SDK cho Kinesis.\nChúng ta cũng sẽ chạy câu lệnh node kds-js-sdk.js để ghi dữ liệu vào Kinesis Data Stream.\nChắc chắn răng bạn đã sử đổi REGION và STREAM_NAME đúng với cấu hình của bạn.\nBạn có thể kiếm tra việc viết dữ liệu thành công hay chưa bằng cách truy cập vào Kinesis Data Stream. Chọn Monitor. Trong Incoming data và PutRecords, nếu thành công, bạn sẽ thấy một ít dữ liệu đến trong Kinesis. Vì chúng ta chỉ viết 20 records nên nó có thể sẽ rất nhỏ. Nếu bạn không quá quen thuộc với Javascript, bạn có thể tải đoạn code Python ở đây và thử sử dụng nó.\n"
},
{
	"uri": "/vi/2-prerequiste/",
	"title": "Các bước chuẩn bị ",
	"tags": [],
	"description": "",
	"content": "Tổng quan AWS cloud9 là một môi trường phát triển tích hợp, hoặc IDE. AWS Cloud9 IDE cung cấp một trải nghiệm viết code phong phú với hỗ trợ cho nhiều ngôn ngữ lập trình và trình gỡ lỗi , cùng với một terminal tích hợp sẵn. Nó chứa một tập hợp các công cụ giúp bạn viết code, xây dựng, chạy, kiểm tra và gỡ lỗi phần mềm, nó giúp bạn triển khai phầm mềm trên cloud.\nAmazon Kinesis Data Streams (KDS) là một dịch vụ truyền dữ liệu thời gian thực có khả năng mở rộng lớn và bền vững. KDS có thể liên tục thu thập hàng tỷ byte dữ liệu mỗi giây từ hàng trăm nghìn nguồn như luồng click trên trang web, luồng sự kiện cơ sở dữ liệu, giao dịch tài chính, luồng thông tin trên mạng xã hội, logs IT và sự kiện theo dõi vị trí. Dữ liệu thu thập được sẵn sàng trong vài mili giây để hỗ trợ các ứng dụng phân tích thời gian thực như bảng điều khiển thời gian thực, phát hiện bất thường thời gian thực, định giá động và nhiều ứng dụng khác.\nAWS Identity and Access Management (IAM) Roles là thực thể bạn tạo ra và gắn những quyền nhất định để cho phép các thực thể đáng tin cậy truy cập vào các tài nguyên AWS của bạn. Khi các thực thể được tin cậy của bạn thực hiện assume IAM roles, họ chỉ được cấp quyền theo phạm vi của những IAM roles đó.\nAmazon Simple Storage Service (Amazon S3) là một dịch vụ lưu trữ dạng đối tượng cung cấp khả năng mở rộng, tính sẵn sàng dữ liệu, bảo mật và hiệu năng hàng đầu trong lĩnh vực. Khách hàng ở mọi quy mô và ngành có thể sử dụng Amazon S3 để lưu trữ và bảo vệ bất kỳ lượng dữ liệu nào cho hàng loạt các trường hợp như: data lakes, website, ứng dụng mobile, backup và restore, lưu trữ, ứng dụng doanh nghiệp, thiết bị IoT và phân tích dữ liệu lớn. Amazon S3 cung cấp tính năng quản lý để bạn có thể tối ưu, tổ chức và cấu hình truy cập vào dữ liệu của bạn để đáp ứng từng như cầu nhất định của kinh doanh, tổ chức và các yêu cầu tuân thủ.\nAmazon DynamoDB là một dịch vụ cơ sở dữ liệu được quản lý hoàn toàn bởi AWS cung cấp hiệu suất cao và dự đoán được với khả năng mở rộng mượt mà. DynamoDB cho phép bạn giảm bớt gánh nặng quản lý của việc vận hành và mở rộng cơ sở dữ liệu phân tán để bạn không cần phải bận tâm về việc cung cấp các thiết bị phần cứng, thiết lập và cấu hình, sao chép cơ sở dữ liệu, vá các lỗi phần mềm, hoặc là mở rộng cụm cluster. DynamoDB cũng cung cấp khả năng mã hóa khi lưu trữ giúp loại bỏ gánh nặng và sự phức tạp trong việc bảo vệ các dữ liệu nhạy cảm.\nNội dung Tạo Cloud9 Environment Tạo Kinesis Data Stream Tạo IAM Roles Tạo S3 Buckets Tạo DynamoDB Table "
},
{
	"uri": "/vi/3-writedataintokds/3.3-producerwithstudionotebook/3.3.2-runstudionotebook/",
	"title": "Chạy Studio notebook",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ thực hiện những thứ sau:\nTruy vấn dữ liệu từ S3. Tạo ra một in-memory table cho Kinesis Stream. Bắt đầu chèn dữ liệu từ S3 vào trong Kinesis Stream sử dụng in-memory table. Truy vấn dữ liệu trong Kinesis Stream để kiểm tra dữ liệu đã được chèn thành công hay chưa. Upload file để đọc dữ liệu từ S3 và tải dữ liệu vào Kinesis Stream. Trong Giao diện Apache Zeppelin. Chọn Import note. Chọn Select JSON File/IPYNB File và tải file. Bây giờ chúng ta cần click vào Taxi Trips Data Loading from S3 to Kinesis-1 để bắt đầu. Tải dữ liệu Taxi Trips từ S3 bucket vào trong Kinesis stream. Trong Taxi Trips Data Loading from S3 to Kinesis-1. Bạn cần thay thế với tên S3 bucket của bạn. Sau đó, click vào nút bắt đầu để tạo nyc_yellow_taxi_trip_data table trong AWS Glue Database. Sau khi thực hiện hành động này, một nyc_yellow_taxi_trip_data table sẽ được tạo bên trong AWS Glue Database.\nTiếp theo, chúng ta sẽ bắt đầu Query data from S3. Kết quả sẽ trông như thế này: Tiếp theo, chúng ta cần tạo một in-memory table cho Kinesis Stream. Tiếp theo chúng ta sẽ bắt đầu chèn dữ liệu từ S3 vào trong Kinesis Stream sử dụng in-memory tables và thực thi truy vấn. Bây giờ, chúng ta đã hoàn thành việc sử dụng Studio notebook để đọc dữ liệu từ S3 và ghi nó vào Kinesis Data Stream thông qua AWS Glue Database.\nĐể dọn dẹp các table đã tạo, bạn sẽ làm theo các bước còn lại trong Taxi Trips Data Loading from S3 to Kinesis-1.\n"
},
{
	"uri": "/vi/4-consumedatainkds/4.2-consumerwithkcl/",
	"title": "Consume dữ liệu với Kinesis Library Client",
	"tags": [],
	"description": "",
	"content": "Kinesis Client Library để phân tích dữ liệu, lưu trữ, bảng điều khiển thời gian thực và nhiều tính năng khác. Mặc dù bạn có thể sử dụng các Amazon Kinesis API functions để xử lý các luồng dữ liệu trực tiếp, KCL quan tâm đến nhiều nhiệm vụ phức tạp liên quan đến xử lý phân tán và cho phép bạn tập trung vào logic xử lý bản ghi. Ví dụ, KCL có thể tự động cân bằng tải xử lý bản ghi trên nhiều phiên bản, cho phép người dùng kiểm tra bản ghi đã được xử lý và xử lý lỗi phiên bản. KCL hoạt động như một trung gian giữa logic xử lý bản ghi của bạn và Kinesis Data Stream.\nTrong bước này, chúng ta sẽ sử dụng Kinesis Library Client để consume dữ liệu từ Kinesis.\nTạo Consumer code. Truy cập vào Giao diện quản lý AWS Cloud9 Chọn My environments. Click Create environment. Tải đoạn code ví dụ có Kinesis Client Library ở đây và unzip trên máy của bạn.\nTrong Cloud9 environment.\nChọn File.\nChọn Upload Local Files.\nChọn Select folder và upload file mà bạn vừa tải xuông.\nMột khi code được upload thành công, mở thư mục và chúng ta sẽ thấy như thế này:\nChúng ta sẽ chạy câu lênh cd kinesis-kcl-example/kcl-app để truy cập vào trong thư mục kcl-app. Chúng ta sẽ chạy 2 câu lệnh bên dưới để cài đặt môi trường: sudo yum install maven -y mvn clean compile assembly\\:single Tiếp theo chúng ta sẽ thực hiện các câu lệnh bên dưới để tạo ra các biến môi trường: export STREAM_NAME=\u0026lt;your-kinesis-stream\u0026gt; export AWS_REGION=\u0026lt;your-aws-region\u0026gt; export APPLICATION_NAME=ImmersiondayKCLConsumer Để consume dữ liệu từ Kinesis Data Stream, chúng ta sẽ thực thi câu lệnh sau: java -jar target/kcl-app-1.0-SNAPSHOT-jar-with-dependencies.jar để bắt đầu consume. Để viết dữ liệu vào Kinesis Data Stream, chúng ta có thể sử dụng python CDK function trong Cloud9 Environment. Mở terminal mới. Chạy câu lệnh python3 kds-py-sdk.py để bắt đầu viết dữ liệu. Chuyển đến giao diện quản lý DynamoDB bạn sẽ thấy một table với tên ImmersiondayKCLConsumer được tạo ra khi chạy KCL application.\nKhi bạn đang chạy KCL, nó sẽ mặc định xuất các chỉ số thống kê đến Amazon CloudWatch. Truy cập vào CloudWatch Metrics và chọn không gian tên ImmersiondayKCLConsumer. Bạn sẽ thấy các chỉ số thống kê KCL-Application, Worker và Shard.\n"
},
{
	"uri": "/vi/5-consumerwithkdf/5.2-createdb/",
	"title": "Tạo AWS Glue và AWS Athena",
	"tags": [],
	"description": "",
	"content": "AWS Glue là một dịch vụ quản lý hoàn toàn cho việc trích xuất, biến đổi và tải (ETL) dữ liệu, giúp việc chuẩn bị và tải dữ liệu cho phân tích trở nên dễ dàng. Nó tự động hóa các nhiệm vụ khám phá dữ liệu, chuyển đổi, ánh xạ và lập lịch công việc tốn nhiều thời gian và công sức.\nAWS Athena là một dịch vụ truy vấn tương tác giúp phân tích dữ liệu trong Amazon S3 dễ dàng bằng SQL tiêu chuẩn. Nó cho phép bạn chạy các truy vấn tùy ý trên các bộ dữ liệu lớn được lưu trữ trong S3 mà không cần công việc ETL phức tạp hoặc cơ sở hạ tầng kho dữ liệu.\nAWS Glue và AWS Athena thường được sử dụng cùng nhau trong kiến trúc data lakes. AWS Glue có thể được sử dụng để chuẩn bị và biến đổi dữ liệu, tải nó vào Amazon S3 và đăng ký nó trong AWS Glue Data Catalog. Khi dữ liệu đã được lưu trong S3 và được đăng ký trong AWS Glue Data Catalog, bạn có thể sử dụng AWS Athena để chạy các truy vấn SQL tùy ý trên dữ liệu, giúp các nhà phân tích dữ liệu và các nhà khoa học dữ liệu có thể thu được thông tin mà không cần cơ sở hạ tầng kho dữ liệu chuyên dụng.\nTrong bước này, chúng ta cần tạo một AWS Glue database và AWS Athena.\nChung ta sẽ sử dụng kinesis-glue-db database mà chúng ta đã tạo trong những bước trước. Tạo Athena. Truy cập vào Giao diện quản trị AWS Athena. Chọn Query editor. Chọn Settings. Click Manage. Trong Manage settings. Chọn S3 bucket, ví dụ: s3://kinesis-data-firehose-lab/nyctaxitrips. Click Save. Sau đó, chúng ta sẽ chuyển đến Editor. Chọn Editor. At Database, choose kinesis-glue-db. Sao chép và dán câu lệnh SQL bên dưới: CREATE EXTERNAL TABLE `nyctaxitrips` ( `id` string, `vendorId` int, `pickupDate` string, `dropoffDate` string, `passengerCount` int, `pickupLongitude` double, `pickupLatitude` double, `dropoffLongitude` double, `dropoffLatitude` double, `storeAndFwdFlag` string, `gcDistance` double, `tripDuration` int, `googleDistance`int, `googleDuration`int, `source`string ) PARTITIONED BY ( `year` string, `month` string, `day` string, `hour` string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; LOCATION \u0026#39;s3://\u0026lt;\u0026lt;BUCKET-NAME\u0026gt;\u0026gt;/nyctaxitrips/\u0026#39; Click Run.\nKết quả sẽ trông như thế này:\nSau khi thực thi câu lệnh, một table sẽ được tạo ra trong kinesis-glue-db database. Truy cập vào kinesis-glue-db database. Chúng ta sẽ thấy như thế này: "
},
{
	"uri": "/vi/2-prerequiste/2.2-createkinesisdatastream/",
	"title": "Tạo Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Streams (KDS) là một dịch vụ truyền dữ liệu thời gian thực có khả năng mở rộng lớn và bền vững. KDS có thể liên tục thu thập hàng tỷ byte dữ liệu mỗi giây từ hàng trăm nghìn nguồn như luồng click trên trang web, luồng sự kiện cơ sở dữ liệu, giao dịch tài chính, luồng thông tin trên mạng xã hội, logs IT và sự kiện theo dõi vị trí. Dữ liệu thu thập được sẵn sàng trong vài mili giây để hỗ trợ các ứng dụng phân tích thời gian thực như bảng điều khiển thời gian thực, phát hiện bất thường thời gian thực, định giá động và nhiều ứng dụng khác.\nTrong bước này, chúng ta cần tạo một Kinesis Data Stream.\nTạo Kinesis Data Stream Truy cập vào AWS Kinesis Console. Chọn Data streams. Click Create data stream. Trong Create data stream. Nhập kinesis-stream. Kéo xuống và click Create data stream. "
},
{
	"uri": "/vi/4-consumedatainkds/4.3-consumerwithamazonmdf/4.3.2-createstudionotebook/",
	"title": "Tạo Studio Notebook",
	"tags": [],
	"description": "",
	"content": "Amazon Managed Service for Apache Flink (rút gọn thành Amazon MSK for Apache Flink hoặc đơn giản là MSK for Flink) là một dịch vụ quản lý hoàn toàn được cung cấp bởi Amazon Web Services (AWS) giúp đơn giản hóa quá trình chạy ứng dụng Apache Flink trên AWS cloud. Apache Flink là một framework xử lý luồng mã nguồn mở có thể được sử dụng cho phân tích thời gian thực, ứng dụng dựa trên sự kiện và nhiều hơn nữa.\nVới Amazon Managed Service for Apache Flink, bạn có thể sử dụng Java, Scala, Python hoặc SQL để xử lý và phân tích dữ liệu luồng. Dịch vụ cho phép bạn soạn thảo và chạy mã đối với nguồn luồng và nguồn tĩnh để thực hiện phân tích chuỗi thời gian, cung cấp bảng điều khiển thời gian thực và các chỉ số.\nTrong bước này, chúng ta cần tạo một Studio Notebook để bắt đầu viết dữ liệu từ Kinesis Data Stream đến Amazon OpenSearch Service.\nTạo Studio notebooks Truy cập vào Giao diện quả lý AWS Kinesis. Chọn Managed Apache Flink. Trong Managed Apache Flink. Chọn Studio notebooks. Click Create Studio notebook. Trong Create Studio notebook. Chọn Create with custom settings. Nhập kinesis-studio-mdf. Kéo xuống và click Next. Trong bước IAM permissions. Chọn Choose from IAM roles that Managed Service for Apache Flink can assume. Tại Service role, chọn kinesis-studio-lab-role. Tại AWS Glue database, chọn kinesis-glue-db. Click Next. Tại Custom connectors. Click Add custom connector. Chọn kinesis-amazon-msk-flink-lab. Tại Path to S3 object, nhập flink-sql-connector/flink-sql-connector-elasticsearch7_2.12-1.13.2.jar. Click Save changes. Sau đó, chúng ta chọn Next và Create Studio notebook. Sau khi tạo thành công, chúng ta sẽ chọn kinesis-studio-mdf. Click Run để khởi động Studio Notebook. Để truy cập vào Studio Notebook, chúng ta click vào Open in Apache Zeppelin. "
},
{
	"uri": "/vi/3-writedataintokds/3.2-producerwithkpl/",
	"title": "Viết dữ liệu với Kinesis Producer Client",
	"tags": [],
	"description": "",
	"content": "The Kinesis Producer Library (KPL) cải thiện khả năng tiếp nhận dữ liệu được đề cập trong phần AWS SDK Kinesis API của workshop này, cho phép các lập trình viên có thể đạt được hiệu năng cao hơn và tối ưu với khả năng thử lại và xử lý lỗi, sử dụng các shards bên trong các luồng dữ liệu để tối ưu khả năng xử lý trên giây.\nTrong bước này, chúng ta sẽ sử dụng Kinesis Producer Library (KPL) để viết dữ liệu vào trong Kinesis Data Stream.\nTạo Producer code Truy cập vào AWS Cloud9 Console. Chọn My environments. Click Create environment. Tải đoạn code cho Kinesis Producer Library ở đây và unzip chúng trên máy của bạn.\nTrong Cloud9 environment.\nChọn File.\nChọn Upload Local Files.\nChọn Select folder và tải lên file mà bạn vừa tải xuống.\nMột khi code đã được tải lên thành công, hãy mở thư mục bên trong.\nThực thi câu lệnh cd kinesis-producer-library-examples-master/.\nThực thi câu lệnh mvn clean compile package. This will compile and package a jar containing all code within the project we are working with. The jar will be generated and placed in the target/ folder of your project workspace.\nChúng ta sẽ thực thi câu lệnh java -cp target/amazon-kinesis-replay-1.0-SNAPSHOT.jar A_SimpleProducer để chạy hàm và bắt đầu viết dữ liệu vào Kinesis Data Stream, bấm Ctrl C để dừng.\nChuyển đến Kinesis Data Stream của chúng ta và đợi một vài phút. Chọn Monitor. Chúng ta có thể theo dõi dữ liệu được viết vào trong Incoming data và PutRecords. "
},
{
	"uri": "/vi/4-consumedatainkds/4.3-consumerwithamazonmdf/",
	"title": "Consume dữ liệu với AWS Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "Tổng quan Amazon Managed Service for Apache Flink (rút gọn thành Amazon MSK for Apache Flink hoặc đơn giản là MSK for Flink) là một dịch vụ quản lý hoàn toàn được cung cấp bởi Amazon Web Services (AWS) giúp đơn giản hóa quá trình chạy ứng dụng Apache Flink trên AWS cloud. Apache Flink là một framework xử lý luồng mã nguồn mở có thể được sử dụng cho phân tích thời gian thực, ứng dụng dựa trên sự kiện và nhiều hơn nữa.\nVới Amazon Managed Service for Apache Flink, bạn có thể sử dụng Java, Scala, Python hoặc SQL để xử lý và phân tích dữ liệu luồng. Dịch vụ cho phép bạn soạn thảo và chạy mã đối với nguồn luồng và nguồn tĩnh để thực hiện phân tích chuỗi thời gian, cung cấp bảng điều khiển thời gian thực và các chỉ số.\nTrong bước này, chúng ta sẽ consume dữ liệu với AWS Managed Service for Apache Flink.\nNội dung Tạo OpenSearch Service Tạo Studio Notebook Truyền dữ liệu và xác thực đầu ra "
},
{
	"uri": "/vi/2-prerequiste/2.3-createiamrole/",
	"title": "Tạo IAM Roles",
	"tags": [],
	"description": "",
	"content": "AWS Identity and Access Management (IAM) Roles là thực thể bạn tạo ra và gắn những quyền nhất định để cho phép các thực thể đáng tin cậy truy cập vào các tài nguyên AWS của bạn. Khi các thực thể được tin cậy của bạn thực hiện assume IAM roles, họ chỉ được cấp quyền theo phạm vi của những IAM roles đó.\nTrong bước này, chúng ta cần tạo IAM Roles để cấp quyền cho một vài dịch vụ sử dụng trong bài lap này.\nTạo IAM Policy Truy cập vào AWS IAM Console. Chọn Policies. Click Create policy. Trong bước Specify permission. Chọn Json. Sao chép và dán đoạn code bên dưới: Bạn cần thay thế , và tên dịch vụ tương ứng.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogGroups\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogStreams\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCloudwatchLogs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/hive\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadConnection\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetConnection\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:connection/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:DeleteTable\u0026#34;, \u0026#34;glue:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:table/kinesis-glue-db/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VPCReadOnlyPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeDhcpOptions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadWriteCode\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-analytics-placeholder-s3-bucket/kinesis-analytics-placeholder-s3-path/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:ResourceAccount\u0026#34;: \u0026#34;\u0026lt;your-aws-account-id\u0026gt;\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;DescribeApplication\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesisanalytics:DescribeApplication\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesisanalytics:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:application/kinesis-notebook-*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadCustomArtifact\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-studio-notebook-lab\u0026#34;, \u0026#34;arn:aws:s3:::kinesis-studio-notebook-lab/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetUserDefinedFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:userDefinedFunction/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesis:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next. Trong bước Review and create. Nhập kinesis-studio-lap-policy. Kéo xuống và click Create. Tạo IAM Role Truy cập vào AWS IAM Console. Chọn Roles. Click Create role. Trong bước Specify trusted entity. Chọn AWS service. Tại service or usecase, chọn Kinesis. Tại usecase, chọn Kinesis Analystics. Trong bước Add permissions. Tìm kiếm và chọn kinesis-studio-lap-policy policy. Click Next. Trong bước Review and create. Nhập kinesis-studio-lap-role. Kéo xuống và click Create. Làm theo các bước như trên, chúng ta cần tạo thêm IAM Role với tên kinesis-amazon-msk-flink-lab-role để tiêu thụ dữ liệu với Amazon Managed Service for Apache Flink.\nPolicy sẽ như thế này:\nBạn cần thay thế , và tên dịch vụ tương ứng.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogGroups\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;ListCloudwatchLogStreams\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;PutCloudwatchLogs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/hive\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueReadConnection\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetConnection\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:connection/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:UpdateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:DeleteTable\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:table/kinesis-glue-db/*\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueDatabase\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VPCReadOnlyPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeDhcpOptions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadWriteCode\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-analytics-placeholder-s3-bucket/kinesis-analytics-placeholder-s3-path/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:ResourceAccount\u0026#34;: \u0026#34;\u0026lt;your-aws-account-id\u0026gt;\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;DescribeApplication\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesisanalytics:DescribeApplication\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesisanalytics:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:application/kinesis-studio-mdf\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadCustomArtifact\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::kinesis-amazon-msk-flink-lab/flink-sql-connector/flink-sql-connector-elasticsearch7_2.12-1.13.2.jar\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;glue:GetUserDefinedFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:catalog\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:database/kinesis-glue-db\u0026#34;, \u0026#34;arn:aws:glue:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:userDefinedFunction/*\u0026#34; ] } ] } Tạo IAM Role cho Lambda Function Chúng ta cũng cần tạo một IAM Role với tên kinesis-lambda-lab-role cho Lambda function có thể tích hợp với Kinesis Data Stream để tiêu thụ dữ liệu. Policy sẽ như thế này: Bạn cần thay thế , và tên dịch vụ tương ứng.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:log-group:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:UpdateTable\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:table/kinesisAggs\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:ListContributorInsights\u0026#34;, \u0026#34;dynamodb:ListGlobalTables\u0026#34;, \u0026#34;dynamodb:ListTables\u0026#34;, \u0026#34;dynamodb:ListBackups\u0026#34;, \u0026#34;dynamodb:ListExports\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesis:GetRecords\u0026#34;, \u0026#34;kinesis:GetShardIterator\u0026#34;, \u0026#34;kinesis:DescribeStream\u0026#34;, \u0026#34;kinesis:DescribeStreamSummary\u0026#34;, \u0026#34;kinesis:ListShards\u0026#34;, \u0026#34;kinesis:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesis:\u0026lt;your-aws-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:stream/kinesis-stream\u0026#34; } ] } "
},
{
	"uri": "/vi/5-consumerwithkdf/5.3-createkdf/",
	"title": "Tạo Kinesis Data Firehose",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Firehose là cách dễ dàng nhất để tải các luồng dữ liệu vào các công cụ lưu trữ và phân tích. Nó có thể bắt, chuyển đổi và tải các luồng dữ liệu vào Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, và Splunk, cho phép phân tích gần thời gian thực với các công cụ thông minh kinh doanh và bảng điều khiển bạn đang sử dụng ngày hôm nay. Đây là một dịch vụ được quản lý hoàn toàn tự động co dãn để phù hợp với lưu lượng dữ liệu của bạn và không đòi hỏi việc quản lý liên tục. Nó cũng có thể gom nhóm, nén và mã hóa dữ liệu trước khi tải, giảm thiểu lượng lưu trữ được sử dụng ở điểm đến và tăng cường bảo mật.\nTrong bước này, chúng ta cần tạo một Kinesis Data Firehose để consume dữ liệu từ Kinesis Data Stream.\nTạo Kinesis Data Firehose. Truy cập vào Giao diện quản lý AWS Kinesis. Chọn Amazon Data Firehose. Trong giao diện quản lý Amazon Data Firehose. Click Create Firehose stream. Trong Create Firehose stream. Tại Source, chọn Amazon Kinesis Data Stream. Chọn kinesis-stream. At Transform and convert records. Chọn Turn on data transformation. Chọn dataTransformHandler function. Chọn Convert record format. Chọn Apache Parquet. Tại AWS Glue region, chọn AWS Glue region của bạn. Chọn kinesis-glue-db. Chọn nyctaxitrips. Tại Destination settings. Chọn kinesis-data-firehose-lab bucket. Tại S3 bucket prefix, sao chép và dán: nyctaxitrips/year=!{timestamp:YYYY}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/. Tại S3 bucket error output prefix, sao chép và dán: nyctaxitripserror/!{firehose:error-output-type}/year=!{timestamp:YYYY}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/. Tại Buffer hints, compression, file extension and encryption. Chỉnh sửa Buffer interval xuống 60 giây. Kéo xuống và click Create Firehose stream. "
},
{
	"uri": "/vi/4-consumedatainkds/4.3-consumerwithamazonmdf/4.3.3-streamdata/",
	"title": "Truyền dữ liệu và xác thực đầu ra",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ bắt đầu truyền dữ liệu và xác thực đầu ra.\nChúng ta cần tải Zeppelin Notebook File, mà chúng ta sẽ dùng để đọc và ghi dữ liệu vào OpenSearch Service.\nBắt đầu truyền dữ liệu. Trong những bước trước, chúng ta đã truy cập vào Studio Notebook thông qua Apache Zeppelin. Trong Apache Zeppelin. Chọn Import note. Chọn JSON File/IPYNB File và upload file mà bạn vừa tải xuống. Bắt đầu consume dữ liếu từ Kinesis Data Stream và tải vào OpenSearch Service. Click KDA-OpenSearch. Trong KDA-OpenSearch. Chúng ta sẽ tạo ra một taxi_trips table để dữ dữ liệu từ Kinesis Stream, chúng ta cần click Run button. Chắc chắn bạn đã chọn đúng aws_region và aws_kinesis_name.\nTiếp theo, chúng ta sẽ bắt đầu consume dữ liệu từ Kinesis Data Stream. Bây giờ chúng ta sẽ bắt đầu viết dữ liệu vào Kinesis Data Stream, chúng ta có thể sử dụng python CDK function trong Cloud9 Environment. Sau khi viết dữ liệu vào Kinesis Data Stream, chúng ta sẽ nhìn thấy như thế này: Tiếp theo chúng ta cần tạo một Taxi Statistics Table để lưu dữ liệu tổng hợp được vào Amazon OpenSearch. Click Run button để tạo table. Chắc chắn rằng bạn đã chọn đúng opensearch_service_hosts (Open Search Domain endpoint v2), và username, password cho Admin User mà bạn đã tạo khi cấu hình OpenSearch Domain.\nBây giờ, chúng ta sẽ chạy ô sau để tổng hợp dữ liệu taxi từ bảng taxi_trip và lưu trữ dữ liệu tổng hợp trong OpenSearch Service. Xác thực đầu ra. Truy cập vào Amazon OpenSearch Service Console. Chọn Domains. Chọn kinesis-opensearch-service và mở OpenSearch Dashboards URL. Chúng ta sẽ sử dụng username và password cho Admin User mà bạn đã tạo khi cấu hình OpenSearch Domain.\nTrong OpenSearch Dashboards. Chọn Menu button. Chọn Query Workbench. Trong Query Workbench Sao chép và dán câu lệnh select * from trip_statistics;. Chọn Run. Chúng ta sẽ nhìn thấy như thế này: Sau đó, chúng ta sẽ chuyển đến Dashboards Management. Chọn Index patterns. Click Create index patterns. Trong bước Define an index pattern. Nhập trip_statistics. Click Next step. Trong bước tiếp theo, chúng ta click vào Create pattern. Tiếp theo chúng ta sẽ chuyển đến Visualize. Click Create new visualization. Chọn Gauge và sau đó chọn trip_statistics. Tạo ra một visualization: "
},
{
	"uri": "/vi/3-writedataintokds/",
	"title": "Viết dữ liệu vào Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Tổng quan AWS Kinesis SDK là bộ công cụ phất triển phần mềm (SDKs) được cung cấp bởi Amazon Web Services (AWS) để tương tác với các dịch vụ của Amazon Kinesis\nThe Kinesis Producer Library (KPL) cải thiện khả năng tiếp nhận dữ liệu được đề cập trong phần AWS SDK Kinesis API của workshop này, cho phép các lập trình viên có thể đạt được hiệu năng cao hơn và tối ưu với khả năng thử lại và xử lý lỗi, sử dụng các shards bên trong các luồng dữ liệu để tối ưu khả năng xử lý trên giây.\nStudio notebooks for Managed Service for Apache Flink cho phép bạn truy vấn dữ liệu luồng một cách tương tác thời gian thực, và dễ dàng xây dựng và chạy các ứng dụng xử lý luồng sử dụng SQL tiêu chuẩn, Python và Scala. Với vài cú nhấp chuột trên bảng điều khiển Quản lý AWS, bạn có thể khởi chạy một serverless notebook để truy vấn các luồng dữ liệu và nhận kết quả trong vài giây.\nTrong bước này, chúng ta sẽ thực hiện viết dữ liệu vào Kinesis Data Stream.\nNội dung Viết dữ liệu với SDK Viết dữ liệu với Kinesis Producer Library Viết dữ liệu với Studio Notebooks "
},
{
	"uri": "/vi/3-writedataintokds/3.3-producerwithstudionotebook/",
	"title": "Viết dữ liệu với Studio notebooks",
	"tags": [],
	"description": "",
	"content": "Tổng quan Studio notebooks for Managed Service for Apache Flink cho phép bạn truy vấn dữ liệu luồng một cách tương tác thời gian thực, và dễ dàng xây dựng và chạy các ứng dụng xử lý luồng sử dụng SQL tiêu chuẩn, Python và Scala. Với vài cú nhấp chuột trên bảng điều khiển Quản lý AWS, bạn có thể khởi chạy một serverless notebook để truy vấn các luồng dữ liệu và nhận kết quả trong vài giây.\nA notebook là một môi trường phát triển dựa trên web. With notebooks, bạn có một trải nghiệm phát triển tương tác đơn giản kết hợp với các khả năng tiên tiến được cung cấp bởi Apache Flink. Studio notebooks sử dụng notebook được cung cấp bởi Apache Zeppelin, và sử dụng Apache Flink làm động cơ xử lý luồng. Studio notebooks tinh giản kết hợp những công nghệ này để làm cho phân tích tiên tiến trên luồng dữ liệu dễ tiếp cận cho các nhà phát triển ở mọi trình độ.\nApache Zeppelin cung cấp cho Studio notebooks một bộ công cụ phân tích đầy đủ, bao gồm:\nTrực quan hóa dữ liệu. Xuất dữ liệu ra các tập tin. Điều khiển định dạng đầu ra để dễ dàng phân tích. Trong bước này, chúng ta sẽ sử dụng Studio notebooks for Managed Service for Apache Flink để viết dữ liệu vào trong Kinesis Data Stream.\nNội dung Tạo Studio Notebook Chạy Studio Notebook "
},
{
	"uri": "/vi/4-consumedatainkds/",
	"title": "Consume từ Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Overview AWS Lambda là một dịch vụ tính toán cho phép bạn chạy mã mà không cần cung cấp hoặc quản lý máy chủ. Lambda chạy code của bạn trên một cơ sở hạ tầng tính toán có khả năng cao và thực hiện tất cả các công việc quản lý tài nguyên tính toán, bao gồm bảo trì máy chủ và hệ điều hành, cung cấp và tự động mở rộng dung lượng, và ghi log. Với Lambda, tất cả những gì bạn cần làm là cung cấp mã của mình trong một trong các thời gian chạy ngôn ngữ mà Lambda hỗ trợ.\nKinesis Client Library để phân tích dữ liệu, lưu trữ, bảng điều khiển thời gian thực và nhiều tính năng khác. Mặc dù bạn có thể sử dụng các Amazon Kinesis API functions để xử lý các luồng dữ liệu trực tiếp, KCL quan tâm đến nhiều nhiệm vụ phức tạp liên quan đến xử lý phân tán và cho phép bạn tập trung vào logic xử lý bản ghi. Ví dụ, KCL có thể tự động cân bằng tải xử lý bản ghi trên nhiều phiên bản, cho phép người dùng kiểm tra bản ghi đã được xử lý và xử lý lỗi phiên bản. KCL hoạt động như một trung gian giữa logic xử lý bản ghi của bạn và Kinesis Data Stream.\nAmazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\nAmazon Managed Service for Apache Flink (shortened as Amazon MSK for Apache Flink or simply MSK for Flink) is a fully managed service provided by Amazon Web Services (AWS) that simplifies the process of running Apache Flink applications on the AWS cloud. Apache Flink is an open-source stream processing framework that can be used for real-time analytics, event-driven applications, and more.\nWith Amazon Managed Service for Apache Flink, you can use Java, Scala, Python, or SQL to process and analyze streaming data. The service enables you to author and run code against streaming sources and static sources to perform time-series analytics, feed real-time dashboards, and metrics.\nIn this step, we will perform consume data from the Kinesis Data Stream.\nContent: Consume data with Lambda Consume data with Kinesis Client Library Consume data with Amazon Managed Service for Apache Flink "
},
{
	"uri": "/vi/2-prerequiste/2.4-creates3bucket/",
	"title": "Tạo S3 Buckets",
	"tags": [],
	"description": "",
	"content": "Amazon Simple Storage Service (Amazon S3) là một dịch vụ lưu trữ dạng đối tượng cung cấp khả năng mở rộng, tính sẵn sàng dữ liệu, bảo mật và hiệu năng hàng đầu trong lĩnh vực. Khách hàng ở mọi quy mô và ngành có thể sử dụng Amazon S3 để lưu trữ và bảo vệ bất kỳ lượng dữ liệu nào cho hàng loạt các trường hợp như: data lakes, website, ứng dụng mobile, backup và restore, lưu trữ, ứng dụng doanh nghiệp, thiết bị IoT và phân tích dữ liệu lớn. Amazon S3 cung cấp tính năng quản lý để bạn có thể tối ưu, tổ chức và cấu hình truy cập vào dữ liệu của bạn để đáp ứng từng như cầu nhất định của kinh doanh, tổ chức và các yêu cầu tuân thủ.\nTrong bước này, chúng ta cần tạo S3 bucket để lưu những dữ liệu yêu cầu cho bài lab này.\nTạo S3 buckets for Studio Notebook Truy cập vào AWS S3 Console. Chọn Buckets. Click Create bucket. Trong Create bucket. Nhập kinesis-studio-notebook-lab. Kéo xuống và click Create bucket. Tiếp theo, hãy tải Taxi Ride Data file mà chúng ta sẽ sử dụng để tải dữ liệu vào Kinesis.\nUpload vào kinesis-studio-notebook-lab bucket file mà bạn vừa tải.\nChuyển vào trong kinesis-studio-notebook-lab.\nClick Upload.\nTrong Upload. Upload file mà bạn vừa tải. Click Upload. Tạo S3 buckets for Amazon Managed Service for Apache Flink Truy cập vào AWS S3 Console. Chọn Buckets. Click Create bucket. Trong Create bucket. Nhập kinesis-amazon-msk-flink-lab. Kéo xuống và click Create bucket. Trong kinesis-amazon-msk-flink-lab. Tạo flink-sql-connector. Tải và upload Flink-sql-connector-elasticsearch file. Tạo S3 buckets for Kinesis Data Firehose Làm theo những bước trên, chúng ta sẽ tạo một bucket với tên là kinesis-data-firehose-lab hoặc bất kỳ tên nào khác mà bạn muốn. Chúng ta cũng cần tạo một folder với tên nyctaxitrips/ bên trong kinesis-data-firehose-lab bucket. "
},
{
	"uri": "/vi/5-consumerwithkdf/5.4-streamdata/",
	"title": "Truyền dữ liệu và xác thực đầu ra",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ bắt đầu truyền dữ liệu và xác thực đầu ra.\nViết dữ liệu vào Kinesis Data Stream. Truy cập vào Cloud9 Environment. Chúng ta sẽ dùng python CDK function để viết dữ liệu vào Kinesis Data Stream. Nếu bạn chưa có python CDK function, Tai code ở đây và upload vào Cloud9 Environment. Chúng ta cần thực thi câu lênh pip install boto3 để cài đặt thư viện boto3. Sau đó, chúng ta sẽ thực thi câu lênh python3 kds-py-sdk.py để viết dữ liệu vào Kinesis Data Stream. Kết quả sẽ như bên dưới: Xác thực đầu ra. Truy cập vào giao diện quả lý Kinesis và click vào “nyc-taxi-trips” Data Firehose. Chúng ta sẽ nhìn thấy như này trong Monitor. Truy cập vào kinesis-data-firehose-lab bucket để xem những dữ liệu nào được ghi vào. Chọn một trong những parquet file. Click Action. Chọn Query with S3 Select. Trong Query with S3 Select. Tại Format, chọn Apache Parquet. Tại Output settings, chọn CSV và Comma. Tiếp theo, chúng ta sẽ click Run SQL query. Kết quả sẽ như thế này: Sau đó chúng ta sẽ truy cập vào Athena Query editor. Tại nyctaxitrips talbe, click vào dấu ba chấm. Chọn Load partitions. Nó sẽ tạo và thực thi một Query mới.\nSau đó, chúng ta sẽ tạo ra một Query mới. Tại nyctaxitrips talbe, click vào dấu ba chấm. Chọn Preview Table. Nó sẽ tạo và thực thi một Query mới.\nKết quả sẽ như bên dưới: "
},
{
	"uri": "/vi/5-consumerwithkdf/",
	"title": "Consume dữ liêu với Kinesis Data Firehose",
	"tags": [],
	"description": "",
	"content": "Tổng quan Amazon Kinesis Data Firehose là cách dễ dàng nhất để tải các luồng dữ liệu vào các công cụ lưu trữ và phân tích. Nó có thể bắt, chuyển đổi và tải các luồng dữ liệu vào Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, và Splunk, cho phép phân tích gần thời gian thực với các công cụ thông minh kinh doanh và bảng điều khiển bạn đang sử dụng ngày hôm nay. Đây là một dịch vụ được quản lý hoàn toàn tự động co dãn để phù hợp với lưu lượng dữ liệu của bạn và không đòi hỏi việc quản lý liên tục. Nó cũng có thể gom nhóm, nén và mã hóa dữ liệu trước khi tải, giảm thiểu lượng lưu trữ được sử dụng ở điểm đến và tăng cường bảo mật.\nTrong bước này, chúng ta sẽ sử dụng Amazon Kinesis Data Firehose để consuem dữ liệu từ Amazon Kinesis Data Stream, chúng ta sẽ sử dụng AWS Lambda để chuyển đổi dữ liệu trước khi gửi nó đến AWS Glue và AWS Athena.\nNội dung Tạo Lambda function Tạo Glue database và Athena Tạo Kinesis Data Firehose Truyền dữ liệu và xác thực đầu ra "
},
{
	"uri": "/vi/2-prerequiste/2.5-createdynamodb/",
	"title": "Tạo DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB là một dịch vụ cơ sở dữ liệu được quản lý hoàn toàn bởi AWS cung cấp hiệu suất cao và dự đoán được với khả năng mở rộng mượt mà. DynamoDB cho phép bạn giảm bớt gánh nặng quản lý của việc vận hành và mở rộng cơ sở dữ liệu phân tán để bạn không cần phải bận tâm về việc cung cấp các thiết bị phần cứng, thiết lập và cấu hình, sao chép cơ sở dữ liệu, vá các lỗi phần mềm, hoặc là mở rộng cụm cluster. DynamoDB cũng cung cấp khả năng mã hóa khi lưu trữ giúp loại bỏ gánh nặng và sự phức tạp trong việc bảo vệ các dữ liệu nhạy cảm.\nTrong bước này, chúng ta sẽ tạo một DynamoDB table để lưu trữ dữ liệu từ Kinesis.\nTạo DynamoDB Table Truy cập vào AWS DynamoDB Console. Chọn Tables. Click Create table. Trong Create table. Nhập kinesisAggs. Tại Partition key, nhập vendorId. Kéo xuống và click Create table. Khi tạo một Bảng và xác định lược đồ DynamoDB của bạn, một trong những tùy chọn đầu tiên mà bạn sẽ được hỏi là chỉ định Khóa Phân vùng (Partition Key) và Khóa Sắp xếp (Sort Key) của Bảng của bạn. Đây là một quyết định quan trọng có ảnh hưởng đến cách mà các bản ghi trong bảng của bạn có thể được truy cập.\nA DynamoDB Partition Key được sử dụng như là primary key để phân vùng dữ liệu của bạn trên nhiều nút lưu trữ của DynamoDB. Đây là một thành phần bắt buộc khi thiết lập một bảng DynamoDB. Sorted Key giúp phân phối dữ liệu của bạn trên các phân vùng khác nhau.\nA DynamoDB Sorted Key là một thuộc tính tùy chọn cho phép sắp xếp các mục bên trong mỗi phân vùng. Bằng cách chỉ định một Sorted Key, bạn cho phép DynamoDB sắp xếp các bản ghi bên trong một phân vùng dựa trên giá trị của Sorted Key.\n"
},
{
	"uri": "/vi/6-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ làm theo những bước sau để dọn dẹp tài nguyên mà chúng ta đã tạo trong bài lab này.\nXóa Kinesis Data Stream Truy cập vào AWS Kinesis Console. Chọn Data streams. Chọn kinesis-stream. Chọn Actions. Chọn Delete. Xóa Studio Notebook Truy cập vào AWS Managed Apache Flink Console. Chọn Studio notebooks. Chọn kinesis-studio-mdf. Chọn Actions. Chọn Delete. Xóa AWS Glue Database Truy cập vào AWS Glue Console. Chọn Databases. Chọn kinesis-glue-db. Chọn Delete. Xóa DynamoDB Truy cập vào AWS DynamoDB Console. Chọn Tables. Chọn kinesisAggs và ImmersiondayKCLConsumer. Click Delete. Xóa S3 Buckets Truy cập vào AWS S3 Console. Chọn Buckets. Chọn kinesis-data-firehose-lab. Trước khi xóa bucket, chúng ta cần phải làm rỗng nó\nChọn Empty Enter permanently delete. After that, we will delete kinesis-data-firehose-lab bucket, chọn kinesis-data-firehose-lab và click Delete. Chúng ta sẽ làm tương tự với kinesis-amazon-msk-flink-lab và kinesis-studio-notebook-lab.\nXóa IAM Roles và IAM Policies Truy cập vào AWS IAM Console Chọn Roles Tìm kiếm và chọn kinesis-studio-lab-role Click Delete After that, we will chọn Policies. Tìm kiếm và chọn kinesis-studio-lab-policy. Click Delete. Chúng ta sẽ làm tương tự với kinesis-lambda-lab-role và kinesis-amazon-msk-flink-lab-role.\nXóa OpenSearch Service Truy cập vào AWS OpenSearch Service Console. Chọn Domains. Chọn kinesis-opensearch-service. Click Delete. "
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]